{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\"\n",
    "Simple implementation of http://arxiv.org/pdf/1502.04623v2.pdf in TensorFlow\n",
    "\n",
    "Example Usage:\n",
    "        python draw.py --data_dir=/tmp/draw --read_attn=True --write_attn=True\n",
    "\n",
    "Author: Eric Jang\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials import mnist\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string(\"data_dir\", \"\", \"\")\n",
    "tf.flags.DEFINE_boolean(\"read_attn\", True, \"enable attention for reader\")\n",
    "tf.flags.DEFINE_boolean(\"write_attn\",True, \"enable attention for writer\")\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MODEL PARAMETERS ##\n",
    "\n",
    "A,B = 28,28 # image width,height\n",
    "img_size = B*A # the canvas size\n",
    "enc_size = 256 # number of hidden units / output size in LSTM\n",
    "dec_size = 256\n",
    "read_n = 5 # read glimpse grid width/height\n",
    "write_n = 5 # write glimpse grid width/height\n",
    "read_size = 2*read_n*read_n if FLAGS.read_attn else 2*img_size\n",
    "write_size = write_n*write_n if FLAGS.write_attn else img_size\n",
    "z_size=10 # QSampler output size\n",
    "T=10 # MNIST generation sequence length\n",
    "batch_size=100 # training minibatch size\n",
    "train_iters=10000\n",
    "learning_rate=1e-3 # learning rate for optimizer\n",
    "eps=1e-8 # epsilon for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## BUILD MODEL ##\n",
    "\n",
    "DO_SHARE=None # workaround for variable_scope(reuse=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=(batch_size,img_size)) # input (batch_size * img_size)\n",
    "e=tf.random_normal((batch_size,z_size), mean=0, stddev=1) # Qsampler noise\n",
    "lstm_enc = tf.contrib.rnn.LSTMCell(enc_size, state_is_tuple=True) # encoder Op\n",
    "lstm_dec = tf.contrib.rnn.LSTMCell(dec_size, state_is_tuple=True) # decoder Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x,output_dim):\n",
    "    \"\"\"\n",
    "    affine transformation Wx+b\n",
    "    assumes x.shape = (batch_size, num_features)\n",
    "    \"\"\"\n",
    "    w=tf.get_variable(\"w\", [x.get_shape()[1], output_dim])\n",
    "    b=tf.get_variable(\"b\", [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.matmul(x,w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterbank(gx, gy, sigma2,delta, N):\n",
    "    grid_i = tf.reshape(tf.cast(tf.range(N), tf.float32), [1, -1])\n",
    "    mu_x = gx + (grid_i - N / 2 - 0.5) * delta # eq 19\n",
    "    mu_y = gy + (grid_i - N / 2 - 0.5) * delta # eq 20\n",
    "    a = tf.reshape(tf.cast(tf.range(A), tf.float32), [1, 1, -1])\n",
    "    b = tf.reshape(tf.cast(tf.range(B), tf.float32), [1, 1, -1])\n",
    "    mu_x = tf.reshape(mu_x, [-1, N, 1])\n",
    "    mu_y = tf.reshape(mu_y, [-1, N, 1])\n",
    "    sigma2 = tf.reshape(sigma2, [-1, 1, 1])\n",
    "    Fx = tf.exp(-tf.square((a - mu_x) / (2*sigma2))) # 2*sigma2?\n",
    "    Fy = tf.exp(-tf.square((b - mu_y) / (2*sigma2))) # batch x N x B\n",
    "    # normalize, sum over A and B dims\n",
    "    Fx=Fx/tf.maximum(tf.reduce_sum(Fx,2,keep_dims=True),eps)\n",
    "    Fy=Fy/tf.maximum(tf.reduce_sum(Fy,2,keep_dims=True),eps)\n",
    "    return Fx,Fy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attn_window(scope,h_dec,N):\n",
    "    with tf.variable_scope(scope,reuse=DO_SHARE):\n",
    "        params=linear(h_dec,5)\n",
    "    # gx_,gy_,log_sigma2,log_delta,log_gamma=tf.split(1,5,params)\n",
    "    gx_,gy_,log_sigma2,log_delta,log_gamma=tf.split(params,5,1)\n",
    "    gx=(A+1)/2*(gx_+1)\n",
    "    gy=(B+1)/2*(gy_+1)\n",
    "    sigma2=tf.exp(log_sigma2)\n",
    "    delta=(max(A,B)-1)/(N-1)*tf.exp(log_delta) # batch x N\n",
    "    return filterbank(gx,gy,sigma2,delta,N)+(tf.exp(log_gamma),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## READ ##\n",
    "def read_no_attn(x,x_hat,h_dec_prev):\n",
    "    return tf.concat([x,x_hat], 1)\n",
    "\n",
    "def read_attn(x,x_hat,h_dec_prev):\n",
    "    Fx,Fy,gamma=attn_window(\"read\",h_dec_prev,read_n)\n",
    "    def filter_img(img,Fx,Fy,gamma,N):\n",
    "        Fxt=tf.transpose(Fx,perm=[0,2,1])\n",
    "        img=tf.reshape(img,[-1,B,A])\n",
    "        glimpse=tf.matmul(Fy,tf.matmul(img,Fxt))\n",
    "        glimpse=tf.reshape(glimpse,[-1,N*N])\n",
    "        return glimpse*tf.reshape(gamma,[-1,1])\n",
    "    x=filter_img(x,Fx,Fy,gamma,read_n) # batch x (read_n*read_n)\n",
    "    x_hat=filter_img(x_hat,Fx,Fy,gamma,read_n)\n",
    "    return tf.concat([x,x_hat], 1) # concat along feature axis\n",
    "\n",
    "read = read_attn if FLAGS.read_attn else read_no_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ENCODE ##\n",
    "def encode(state,input):\n",
    "    \"\"\"\n",
    "    run LSTM\n",
    "    state = previous encoder state\n",
    "    input = cat(read,h_dec_prev)\n",
    "    returns: (output, new_state)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"encoder\",reuse=DO_SHARE):\n",
    "        return lstm_enc(input,state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Q-SAMPLER (VARIATIONAL AUTOENCODER) ##\n",
    "\n",
    "def sampleQ(h_enc):\n",
    "    \"\"\"\n",
    "    Samples Zt ~ normrnd(mu,sigma) via reparameterization trick for normal dist\n",
    "    mu is (batch,z_size)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"mu\",reuse=DO_SHARE):\n",
    "        mu=linear(h_enc,z_size)\n",
    "    with tf.variable_scope(\"sigma\",reuse=DO_SHARE):\n",
    "        logsigma=linear(h_enc,z_size)\n",
    "        sigma=tf.exp(logsigma)\n",
    "    return (mu + sigma*e, mu, logsigma, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## DECODER ##\n",
    "def decode(state,input):\n",
    "    with tf.variable_scope(\"decoder\",reuse=DO_SHARE):\n",
    "        return lstm_dec(input, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## WRITER ##\n",
    "def write_no_attn(h_dec):\n",
    "    with tf.variable_scope(\"write\",reuse=DO_SHARE):\n",
    "        return linear(h_dec,img_size)\n",
    "\n",
    "def write_attn(h_dec):\n",
    "    with tf.variable_scope(\"writeW\",reuse=DO_SHARE):\n",
    "        w=linear(h_dec,write_size) # batch x (write_n*write_n)\n",
    "    N=write_n\n",
    "    w=tf.reshape(w,[batch_size,N,N])\n",
    "    Fx,Fy,gamma=attn_window(\"write\",h_dec,write_n)\n",
    "    Fyt=tf.transpose(Fy,perm=[0,2,1])\n",
    "    wr=tf.matmul(Fyt,tf.matmul(w,Fx))\n",
    "    wr=tf.reshape(wr,[batch_size,B*A])\n",
    "    #gamma=tf.tile(gamma,[1,B*A])\n",
    "    return wr*tf.reshape(1.0/gamma,[-1,1])\n",
    "\n",
    "write=write_attn if FLAGS.write_attn else write_no_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STATE VARIABLES ##\n",
    "\n",
    "cs=[0]*T # sequence of canvases\n",
    "mus,logsigmas,sigmas=[0]*T,[0]*T,[0]*T # gaussian params generated by SampleQ. We will need these for computing loss.\n",
    "# initial states\n",
    "h_dec_prev=tf.zeros((batch_size,dec_size))\n",
    "enc_state=lstm_enc.zero_state(batch_size, tf.float32)\n",
    "dec_state=lstm_dec.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## DRAW MODEL ##\n",
    "\n",
    "# construct the unrolled computational graph\n",
    "for t in range(T):\n",
    "    c_prev = tf.zeros((batch_size,img_size)) if t==0 else cs[t-1]\n",
    "    x_hat=x-tf.sigmoid(c_prev) # error image\n",
    "    r=read(x,x_hat,h_dec_prev)\n",
    "    h_enc,enc_state=encode(enc_state,tf.concat([r,h_dec_prev], 1))\n",
    "    z,mus[t],logsigmas[t],sigmas[t]=sampleQ(h_enc)\n",
    "    h_dec,dec_state=decode(dec_state,z)\n",
    "    cs[t]=c_prev+write(h_dec) # store results\n",
    "    h_dec_prev=h_dec\n",
    "    DO_SHARE=True # from now on, share variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## LOSS FUNCTION ##\n",
    "\n",
    "def binary_crossentropy(t,o):\n",
    "    return -(t*tf.log(o+eps) + (1.0-t)*tf.log(1.0-o+eps))\n",
    "\n",
    "# reconstruction term appears to have been collapsed down to a single scalar value (rather than one per item in minibatch)\n",
    "x_recons=tf.nn.sigmoid(cs[-1])\n",
    "\n",
    "# after computing binary cross entropy, sum across features then take the mean of those sums across minibatches\n",
    "Lx=tf.reduce_sum(binary_crossentropy(x,x_recons),1) # reconstruction term\n",
    "Lx=tf.reduce_mean(Lx)\n",
    "\n",
    "kl_terms=[0]*T\n",
    "for t in range(T):\n",
    "    mu2=tf.square(mus[t])\n",
    "    sigma2=tf.square(sigmas[t])\n",
    "    logsigma=logsigmas[t]\n",
    "    kl_terms[t]=0.5*tf.reduce_sum(mu2+sigma2-2*logsigma,1)-.5 # each kl term is (1xminibatch)\n",
    "KL=tf.add_n(kl_terms) # this is 1xminibatch, corresponding to summing kl_terms from 1:T\n",
    "Lz=tf.reduce_mean(KL) # average over minibatches\n",
    "\n",
    "cost=Lx+Lz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## OPTIMIZER ##\n",
    "\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate, beta1=0.5)\n",
    "grads=optimizer.compute_gradients(cost)\n",
    "for i,(g,v) in enumerate(grads):\n",
    "    if g is not None:\n",
    "        grads[i]=(tf.clip_by_norm(g,5),v) # clip gradients\n",
    "train_op=optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN TRAINING ##\n",
    "\n",
    "data_directory = os.path.join(FLAGS.data_dir, \"mnist\")\n",
    "if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "train_data = mnist.input_data.read_data_sets(data_directory, one_hot=True).train # binarized (0-1) mnist data\n",
    "\n",
    "fetches=[]\n",
    "fetches.extend([Lx,Lz,train_op])\n",
    "Lxs=[0]*train_iters\n",
    "Lzs=[0]*train_iters\n",
    "\n",
    "sess=tf.InteractiveSession()\n",
    "\n",
    "saver = tf.train.Saver() # saves variables learned during training\n",
    "tf.global_variables_initializer().run()\n",
    "#saver.restore(sess, \"/tmp/draw/drawmodel.ckpt\") # to restore from model, uncomment this line\n",
    "\n",
    "for i in range(train_iters):\n",
    "        xtrain,_=train_data.next_batch(batch_size) # xtrain is (batch_size x img_size)\n",
    "        feed_dict={x:xtrain}\n",
    "        results=sess.run(fetches,feed_dict)\n",
    "        Lxs[i],Lzs[i],_=results\n",
    "        if i%100==0:\n",
    "                print(\"iter=%d : Lx: %f Lz: %f\" % (i,Lxs[i],Lzs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TRAINING FINISHED ##\n",
    "\n",
    "canvases=sess.run(cs,feed_dict) # generate some examples\n",
    "canvases=np.array(canvases) # T x batch x img_size\n",
    "\n",
    "out_file=os.path.join(FLAGS.data_dir,\"draw_data.npy\")\n",
    "np.save(out_file,[canvases,Lxs,Lzs])\n",
    "print(\"Outputs saved in file: %s\" % out_file)\n",
    "\n",
    "ckpt_file=os.path.join(FLAGS.data_dir,\"drawmodel.ckpt\")\n",
    "print(\"Model saved in file: %s\" % saver.save(sess,ckpt_file))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
